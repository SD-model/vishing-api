
from fastapi import FastAPI, UploadFile, File
import uvicorn
import re
import shutil
import os
import tempfile
import zipfile
import gdown
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
import whisper

app = FastAPI()

# -------- Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: ØªØ­Ù…ÙŠÙ„ Ù…Ù† Google Drive Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯ --------
MODEL_DIR = "vishing_model/FINAL_vishing_detection_model_2025-03-19_22-18-11"
GDRIVE_ZIP_URL = "https://drive.google.com/uc?id=1HDDSV6abkXuLXZSjHvWsh4hsaVtnCGLz"

if not os.path.exists(MODEL_DIR):
    print("ðŸ”„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ù† Google Drive...")
    gdown.download(GDRIVE_ZIP_URL, output="model.zip", quiet=False)
    with zipfile.ZipFile("model.zip", 'r') as zip_ref:
        zip_ref.extractall(MODEL_DIR)
    print("âœ… ØªÙ… ÙÙƒ Ø¶ØºØ· Ø§Ù„Ù†Ù…ÙˆØ°Ø¬!")

tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)
model = AutoModelForSequenceClassification.from_pretrained(MODEL_DIR)
whisper_model = whisper.load_model("base")

# -------- Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„ØªØµÙÙŠØ© Ù„Ù„Ù†ØµÙˆØµ --------
STOPWORDS = set(["ÙÙŠ", "Ø¹Ù„Ù‰", "Ù…Ù†", "Ø¥Ù„Ù‰", "Ø¹Ù†", "Ø£Ù†", "Ù…Ø¹", "ÙƒØ§Ù†", "Ø§Ù„ØªÙŠ", "Ù‡Ø°Ø§", "Ø°Ù„Ùƒ", "Ù…Ø§", "Ù„Ù…", 
                 "Ù„Ù†", "Ù‚Ø¯", "Ø«Ù…", "Ø¥Ø°", "Ø¨Ø¹Ø¯", "Ù‚Ø¨Ù„", "Ø­ØªÙ‰", "ÙƒÙ„", "Ø£ÙŠ", "Ø¨Ù‡", "Ø¥Ù„ÙŠ", "Ø¨ÙŠÙ†", "Ù…Ø«Ù„", "Ø¹Ù†Ø¯", 
                 "Ùˆ", "Ø¨", "Ù", "ÙƒÙ…Ø§", "Ø¥Ù†", "Ø¥Ø°Ø§", "Ù„ÙƒÙ†", "Ø£Ùˆ", "Ø¨Ù„", "Ø£ÙŠØ¶Ø§", "Ø­ÙŠØ«", "ÙƒÙ…Ø§", "Ù„Ø£Ù†", "Ø¥Ù„Ø§", 
                 "Ø¨Ø³Ø¨Ø¨", "Ø¹Ø¨Ø±", "Ø¯Ø§Ø®Ù„", "Ø®Ù„Ø§Ù„", "Ø¯ÙˆÙ†", "Ø­ÙˆÙ„", "Ø­Ø³Ø¨", "Ø¨Ù…Ø§", "Ø¥Ø¶Ø§ÙØ©", "Ø¹Ù„ÙŠÙ‡", "Ù„Ø°Ù„Ùƒ", 
                 "Ø°Ù„Ùƒ", "Ø¨Ù‡Ø°Ø§", "Ù…Ù†Ø°"])

KEYWORDS_TO_REMOVE = set(["Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ…", "ÙˆØ¹Ù„ÙŠÙƒÙ… Ø§Ù„Ø³Ù„Ø§Ù…", "ØµØ¨Ø§Ø­ Ø§Ù„Ø®ÙŠØ±", "Ù…Ø³Ø§Ø¡ Ø§Ù„Ø®ÙŠØ±", "Ø£Ù‡Ù„Ù‹Ø§ ÙˆØ³Ù‡Ù„Ù‹Ø§",
                          "Ù…Ø±Ø­Ø¨Ù‹Ø§", "ÙƒÙŠÙ Ø­Ø§Ù„Ùƒ", "ÙƒÙŠÙ Ø­Ø§Ù„ÙƒÙ…", "ØªØ­ÙŠØ§ØªÙŠ", "ÙŠÙˆÙ… Ø³Ø¹ÙŠØ¯", "ØªØ­ÙŠØ© Ø·ÙŠØ¨Ø© ÙˆØ¨Ø¹Ø¯",
                          "Ù…Ø¹ Ø®Ø§Ù„Øµ Ø§Ù„Ø´ÙƒØ± ÙˆØ§Ù„ØªÙ‚Ø¯ÙŠØ±", "Ø£Ø±Ø¬Ùˆ Ø£Ù† ØªÙƒÙˆÙ† Ø¨Ø®ÙŠØ±", "Ù†Ø£Ù…Ù„ ØªÙØ§Ø¹Ù„ÙƒÙ… Ø§Ù„ÙƒØ±ÙŠÙ…",
                          "ÙÙŠ Ø§Ù†ØªØ¸Ø§Ø± Ø±Ø¯ÙƒÙ… Ø§Ù„ÙƒØ±ÙŠÙ…", "Ø´Ø§ÙƒØ±ÙŠÙ† Ù„ÙƒÙ… Ø­Ø³Ù† ØªØ¹Ø§ÙˆÙ†ÙƒÙ…", "Ø¯Ù…ØªÙ… Ø¨Ø®ÙŠØ±", "ÙƒÙŠÙ Ø§Ù„Ø£Ù…ÙˆØ±",
                          "ÙƒÙ„ Ø¹Ø§Ù… ÙˆØ£Ù†ØªÙ… Ø¨Ø®ÙŠØ±", "Ø¨Ø§Ù„ØªÙˆÙÙŠÙ‚ Ø¥Ù† Ø´Ø§Ø¡ Ø§Ù„Ù„Ù‡", "Ù…Ø§ Ø´Ø§Ø¡ Ø§Ù„Ù„Ù‡", "Ø¬Ø²Ø§Ùƒ Ø§Ù„Ù„Ù‡ Ø®ÙŠØ±Ù‹Ø§",
                          "Ø¨Ø§Ø±Ùƒ Ø§Ù„Ù„Ù‡ ÙÙŠÙƒ", "Ø§Ù„Ù„Ù‡ ÙŠØ¹Ø·ÙŠÙƒ Ø§Ù„Ø¹Ø§ÙÙŠØ©", "Ø§Ù„Ù„Ù‡ ÙŠØ³Ø¹Ø¯Ùƒ", "Ø¹ÙŠØ¯ÙƒÙ… Ù…Ø¨Ø§Ø±Ùƒ"])

# -------- Ø¯Ø§Ù„Ø© ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ --------
def clean_text(text):
    if not isinstance(text, str):
        return ""
    text = re.sub(r"[^Ø€-Û¿\s]", "", text)
    text = re.sub(r"\s+", " ", text).strip()
    text = re.sub(r"[0-9Ù -Ù©]", "", text)
    text = re.sub(r"[\u064B-\u065F]", "", text)
    text = re.sub(r"[Ø¥Ø£Ø¢Ø§]", "Ø§", text)
    text = re.sub(r"Ø©", "Ù‡", text)
    text = re.sub(r"Ù‰", "ÙŠ", text)
    text = re.sub(r"Ø¡", "", text)

    for phrase in KEYWORDS_TO_REMOVE:
        text = text.replace(phrase, "")
    
    tokens = text.split()
    tokens = [t for t in tokens if t not in STOPWORDS]
    return " ".join(tokens)

# -------- ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØª --------
def transcribe_audio(file_path: str) -> str:
    result = whisper_model.transcribe(file_path, language="ar")
    return result["text"]

def predict(text: str):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True)
    with torch.no_grad():
        outputs = model(**inputs)
        predicted_class = torch.argmax(outputs.logits, dim=1).item()
    return "Ø§Ø­ØªÙŠØ§Ù„ÙŠØ©" if predicted_class == 1 else "Ù„ÙŠØ³Øª Ø§Ø­ØªÙŠØ§Ù„ÙŠØ©"

# -------- Endpoint --------
@app.post("/analyze-call")
async def analyze_call(file: UploadFile = File(...)):
    with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as temp_file:
        shutil.copyfileobj(file.file, temp_file)
        temp_file_path = temp_file.name

    raw_text = transcribe_audio(temp_file_path)
    cleaned_text = clean_text(raw_text)
    label = predict(cleaned_text)

    os.remove(temp_file_path)
    return {"text": raw_text, "cleaned": cleaned_text, "label": label}
